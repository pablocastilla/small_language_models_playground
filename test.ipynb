{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\phi-2-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"cuda\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about Sevilla to Browse My List of Things to Do\n",
      "When it comes to choosing a destination for your next vacation, there are many factors to consider. One thing that I know is that many people like to visit Seville, the capital city of Spain. So, if you're looking to plan your trip to this beautiful city, here are some things to keep in mind:\n",
      "\n",
      "  * Explore the historic center of Seville, which is known for its beautiful architecture and delicious food.\n",
      "  * Take a stroll through the vibrant neighborhoods of Barrio Santa Cruz, which are filled with trendy shops, restaurants, and bars.\n",
      "  * Visit the stunning views from the top of the Alhambra palace, which offers breathtaking views of the city and the Mediterranean Sea.\n",
      "  * Check out the beautiful views of the city from the top of the Royal Palace of Seville, which offers panoramic views of the entire city.\n",
      "  * Try some traditional Spanish cuisine, such as paella and tapas, and enjoy the relaxed atmosphere of the city.\n",
      "\n",
      "Overall, Seville is a wonderful destination to visit, with its beautiful architecture, delicious food, and friendly locals. Whether you want to explore the historic center or take in the views from the top of a mountain, there's something for everyone in Seville.<|im_end|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n",
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''Tell me about the city of Sevilla''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
